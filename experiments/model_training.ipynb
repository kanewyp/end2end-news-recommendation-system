{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e89d1e",
   "metadata": {},
   "source": [
    "### 5. Model Training - Content-Based CNN with GloVe Embeddings\n",
    "\n",
    "We'll implement a content-based news recommendation system using:\n",
    "- **CNN architecture** to capture local patterns in news text\n",
    "- **Pre-trained GloVe embeddings** for word representations\n",
    "- **Multi-channel approach** processing titles and abstracts separately\n",
    "- **User interaction data** for training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67ff5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4cef526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA LOADED ===\n",
      "Vocabulary size: 23761\n",
      "Embedding matrix shape: (23762, 100)\n",
      "Number of users: 1001\n",
      "Number of categories: 16\n",
      "Number of subcategories: 190\n",
      "\n",
      "Using device: xpu (Intel XPU)\n",
      "Device type: xpu\n"
     ]
    }
   ],
   "source": [
    "# Load prepared dictionaries and embeddings\n",
    "with open('embeddings/word_dict.pkl', 'rb') as f:\n",
    "    word_dict = pickle.load(f)\n",
    "    \n",
    "with open('embeddings/category_dict.pkl', 'rb') as f:\n",
    "    category_dict = pickle.load(f)\n",
    "\n",
    "with open('embeddings/subcategory_dict.pkl', 'rb') as f:\n",
    "    subcategory_dict = pickle.load(f)\n",
    "\n",
    "with open('embeddings/uid2index.pkl', 'rb') as f:\n",
    "    uid2index = pickle.load(f)\n",
    "\n",
    "# Load GloVe embedding matrix\n",
    "embedding_matrix = np.load('embeddings/embedding.npy')\n",
    "\n",
    "print(f\"\\n=== DATA LOADED ===\")\n",
    "print(f\"Vocabulary size: {len(word_dict)}\")\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "print(f\"Number of users: {len(uid2index)}\")\n",
    "print(f\"Number of categories: {len(category_dict)}\")\n",
    "print(f\"Number of subcategories: {len(subcategory_dict)}\")\n",
    "\n",
    "# Set device with proper priority\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\\nUsing device: {device} (CUDA)\")\n",
    "elif hasattr(torch, 'xpu') and torch.xpu.is_available():\n",
    "    device = torch.device('xpu')\n",
    "    print(f\"\\nUsing device: {device} (Intel XPU)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"\\nUsing device: {device} (CPU)\")\n",
    "\n",
    "print(f\"Device type: {device.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b692ecb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16606 entries, 0 to 16605\n",
      "Data columns (total 11 columns):\n",
      " #   Column                          Non-Null Count  Dtype \n",
      "---  ------                          --------------  ----- \n",
      " 0   Impression_ID                   16606 non-null  int64 \n",
      " 1   User_ID                         16606 non-null  object\n",
      " 2   Time                            16606 non-null  object\n",
      " 3   History                         16332 non-null  object\n",
      " 4   Impressions                     16606 non-null  object\n",
      " 5   Hour                            16606 non-null  int64 \n",
      " 6   DayOfWeek                       16606 non-null  object\n",
      " 7   Clicked_News                    16606 non-null  object\n",
      " 8   Non_Clicked_News                16606 non-null  object\n",
      " 9   First_Clicked_News_Category     16606 non-null  object\n",
      " 10  First_Clicked_News_SubCategory  16606 non-null  object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "news_data=pd.read_csv('news_data.tsv', sep='\\t')\n",
    "behaviours_data=pd.read_csv('behaviours_data.tsv', sep='\\t')\n",
    "behaviours_data.info()\n",
    "\n",
    "# Convert string representations of lists to actual lists\n",
    "for col in ['Clicked_News', 'Non_Clicked_News']:\n",
    "    behaviours_data[col] = behaviours_data[col].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "511eb570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Total training samples: 53979\n",
      "Positive samples: 26993, Negative samples: 26986\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing functions\n",
    "def text_to_sequence(text_tokens, word_dict, max_len=30):\n",
    "    \"\"\"Convert tokenized text to sequence of word indices\"\"\"\n",
    "    sequence = []\n",
    "    for word in text_tokens[:max_len]:  # Truncate to max_len\n",
    "        if word in word_dict:\n",
    "            sequence.append(word_dict[word])\n",
    "        else:\n",
    "            sequence.append(0)  # Unknown word\n",
    "    \n",
    "    # Pad sequence to max_len\n",
    "    while len(sequence) < max_len:\n",
    "        sequence.append(0)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "def prepare_training_data(news_data, behaviour_data):\n",
    "    \"\"\"Prepare training data from news and behaviour data\"\"\"\n",
    "    training_samples = []\n",
    "    \n",
    "    # Get user-news interactions\n",
    "    for _, row in behaviour_data.iterrows():\n",
    "        user_id = row['User_ID']\n",
    "        if user_id not in uid2index:\n",
    "            continue\n",
    "            \n",
    "        user_idx = uid2index[user_id]\n",
    "        \n",
    "        # Positive samples (clicked news)\n",
    "        if row['Clicked_News'] and len(row['Clicked_News']) > 0:\n",
    "            for news_id in row['Clicked_News']:\n",
    "                news_info = news_data[news_data['News_ID'] == news_id]\n",
    "                if len(news_info) > 0:\n",
    "                    training_samples.append({\n",
    "                        'user_idx': user_idx,\n",
    "                        'news_id': news_id,\n",
    "                        'label': 1\n",
    "                    })\n",
    "        \n",
    "        # Negative samples (non-clicked news from impressions)\n",
    "        if row['Non_Clicked_News'] and len(row['Non_Clicked_News']) > 0:\n",
    "            # Sample some negative examples (to balance dataset)\n",
    "            neg_samples = row['Non_Clicked_News'][:len(row['Clicked_News'])] if row['Clicked_News'] else row['Non_Clicked_News'][:5]\n",
    "            for news_id in neg_samples:\n",
    "                news_info = news_data[news_data['News_ID'] == news_id]\n",
    "                if len(news_info) > 0:\n",
    "                    training_samples.append({\n",
    "                        'user_idx': user_idx,\n",
    "                        'news_id': news_id,\n",
    "                        'label': 0\n",
    "                    })\n",
    "    \n",
    "    return training_samples\n",
    "\n",
    "print(\"Preparing training data...\")\n",
    "training_samples = prepare_training_data(news_data, behaviours_data)\n",
    "print(f\"Total training samples: {len(training_samples)}\")\n",
    "\n",
    "# Count positive and negative samples\n",
    "pos_samples = sum(1 for sample in training_samples if sample['label'] == 1)\n",
    "neg_samples = len(training_samples) - pos_samples\n",
    "print(f\"Positive samples: {pos_samples}, Negative samples: {neg_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fe85d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 43183\n",
      "Validation samples: 10796\n",
      "Training batches: 338\n",
      "Validation batches: 85\n"
     ]
    }
   ],
   "source": [
    "# Dataset class for news recommendation\n",
    "class NewsRecommendationDataset(Dataset):\n",
    "    def __init__(self, samples, news_data, word_dict, category_dict, subcategory_dict, max_title_len=30, max_abstract_len=50):\n",
    "        self.samples = samples\n",
    "        self.news_data = news_data.set_index('News_ID')\n",
    "        self.word_dict = word_dict\n",
    "        self.category_dict = category_dict\n",
    "        self.subcategory_dict = subcategory_dict\n",
    "        self.max_title_len = max_title_len\n",
    "        self.max_abstract_len = max_abstract_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        news_id = sample['news_id']\n",
    "        user_idx = sample['user_idx']\n",
    "        label = sample['label']\n",
    "        \n",
    "        # Get news information\n",
    "        try:\n",
    "            news_row = self.news_data.loc[news_id]\n",
    "            \n",
    "            # Convert title and abstract to sequences\n",
    "            title_seq = text_to_sequence(news_row['Title'], self.word_dict, self.max_title_len)\n",
    "            abstract_seq = text_to_sequence(news_row['Abstract'], self.word_dict, self.max_abstract_len)\n",
    "            \n",
    "            # Get category and subcategory indices\n",
    "            category_idx = self.category_dict.get(news_row['Category'], 0)\n",
    "            subcategory_idx = self.subcategory_dict.get(news_row['SubCategory'], 0)\n",
    "            \n",
    "            return {\n",
    "                'user_idx': torch.tensor(user_idx, dtype=torch.long),\n",
    "                'title_seq': torch.tensor(title_seq, dtype=torch.long),\n",
    "                'abstract_seq': torch.tensor(abstract_seq, dtype=torch.long),\n",
    "                'category_idx': torch.tensor(category_idx, dtype=torch.long),\n",
    "                'subcategory_idx': torch.tensor(subcategory_idx, dtype=torch.long),\n",
    "                'label': torch.tensor(label, dtype=torch.float)\n",
    "            }\n",
    "        except KeyError:\n",
    "            # Handle missing news (return zeros)\n",
    "            return {\n",
    "                'user_idx': torch.tensor(user_idx, dtype=torch.long),\n",
    "                'title_seq': torch.zeros(self.max_title_len, dtype=torch.long),\n",
    "                'abstract_seq': torch.zeros(self.max_abstract_len, dtype=torch.long),\n",
    "                'category_idx': torch.tensor(0, dtype=torch.long),\n",
    "                'subcategory_idx': torch.tensor(0, dtype=torch.long),\n",
    "                'label': torch.tensor(label, dtype=torch.float)\n",
    "            }\n",
    "\n",
    "# Create train/validation split\n",
    "train_samples, val_samples = train_test_split(training_samples, test_size=0.2, random_state=42, stratify=[s['label'] for s in training_samples])\n",
    "\n",
    "print(f\"Training samples: {len(train_samples)}\")\n",
    "print(f\"Validation samples: {len(val_samples)}\")\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = NewsRecommendationDataset(train_samples, news_data, word_dict, category_dict, subcategory_dict)\n",
    "val_dataset = NewsRecommendationDataset(val_samples, news_data, word_dict, category_dict, subcategory_dict)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b1fe63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized successfully!\n",
      "Total parameters: 2,974,845\n",
      "Trainable parameters: 2,974,845\n",
      "Model device: xpu:0\n"
     ]
    }
   ],
   "source": [
    "# Content-Based CNN Model\n",
    "class NewsRecommendationCNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_users, num_categories, num_subcategories, \n",
    "                 embed_dim=100, num_filters=128, filter_sizes=[3, 4, 5], \n",
    "                 user_embed_dim=50, category_embed_dim=20, dropout=0.3):\n",
    "        super(NewsRecommendationCNN, self).__init__()\n",
    "        \n",
    "        vocab_size = embedding_matrix.shape[0]\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        # Word embeddings (pre-trained GloVe)\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.word_embedding.weight = nn.Parameter(torch.FloatTensor(embedding_matrix))\n",
    "        self.word_embedding.weight.requires_grad = True  # Fine-tune embeddings\n",
    "        \n",
    "        # User embeddings\n",
    "        self.user_embedding = nn.Embedding(num_users + 1, user_embed_dim)\n",
    "        \n",
    "        # Category embeddings\n",
    "        self.category_embedding = nn.Embedding(num_categories + 1, category_embed_dim)\n",
    "        self.subcategory_embedding = nn.Embedding(num_subcategories + 1, category_embed_dim)\n",
    "        \n",
    "        # CNN layers for title\n",
    "        self.title_convs = nn.ModuleList([\n",
    "            nn.Conv1d(embed_dim, num_filters, kernel_size=k) for k in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        # CNN layers for abstract\n",
    "        self.abstract_convs = nn.ModuleList([\n",
    "            nn.Conv1d(embed_dim, num_filters, kernel_size=k) for k in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Calculate final feature dimension\n",
    "        cnn_output_dim = len(filter_sizes) * num_filters * 2  # title + abstract\n",
    "        total_dim = cnn_output_dim + user_embed_dim + category_embed_dim * 2\n",
    "        \n",
    "        # Final prediction layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(total_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, user_idx, title_seq, abstract_seq, category_idx, subcategory_idx):\n",
    "        batch_size = title_seq.size(0)\n",
    "        \n",
    "        # User embeddings\n",
    "        user_emb = self.user_embedding(user_idx)\n",
    "        \n",
    "        # Category embeddings\n",
    "        cat_emb = self.category_embedding(category_idx)\n",
    "        subcat_emb = self.subcategory_embedding(subcategory_idx)\n",
    "        \n",
    "        # Word embeddings for title and abstract\n",
    "        title_emb = self.word_embedding(title_seq)  # (batch, seq_len, embed_dim)\n",
    "        abstract_emb = self.word_embedding(abstract_seq)\n",
    "        \n",
    "        # Transpose for CNN (batch, embed_dim, seq_len)\n",
    "        title_emb = title_emb.transpose(1, 2)\n",
    "        abstract_emb = abstract_emb.transpose(1, 2)\n",
    "        \n",
    "        # Apply CNN to title\n",
    "        title_conv_outputs = []\n",
    "        for conv in self.title_convs:\n",
    "            conv_out = F.relu(conv(title_emb))  # (batch, num_filters, new_seq_len)\n",
    "            pooled = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)  # (batch, num_filters)\n",
    "            title_conv_outputs.append(pooled)\n",
    "        title_features = torch.cat(title_conv_outputs, dim=1)\n",
    "        \n",
    "        # Apply CNN to abstract\n",
    "        abstract_conv_outputs = []\n",
    "        for conv in self.abstract_convs:\n",
    "            conv_out = F.relu(conv(abstract_emb))\n",
    "            pooled = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)\n",
    "            abstract_conv_outputs.append(pooled)\n",
    "        abstract_features = torch.cat(abstract_conv_outputs, dim=1)\n",
    "        \n",
    "        # Combine all features\n",
    "        combined_features = torch.cat([\n",
    "            user_emb, title_features, abstract_features, cat_emb, subcat_emb\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Apply dropout\n",
    "        combined_features = self.dropout(combined_features)\n",
    "        \n",
    "        # Final prediction\n",
    "        output = self.fc(combined_features)\n",
    "        \n",
    "        return output.squeeze(1)\n",
    "\n",
    "# Initialize model\n",
    "model = NewsRecommendationCNN(\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    num_users=len(uid2index),\n",
    "    num_categories=len(category_dict),\n",
    "    num_subcategories=len(subcategory_dict),\n",
    "    embed_dim=100,\n",
    "    num_filters=128,\n",
    "    filter_sizes=[3, 4, 5],\n",
    "    user_embed_dim=50,\n",
    "    category_embed_dim=20,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model initialized successfully!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acb7a52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training components initialized!\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluation functions\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        # Move batch to device\n",
    "        user_idx = batch['user_idx'].to(device)\n",
    "        title_seq = batch['title_seq'].to(device)\n",
    "        abstract_seq = batch['abstract_seq'].to(device)\n",
    "        category_idx = batch['category_idx'].to(device)\n",
    "        subcategory_idx = batch['subcategory_idx'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(user_idx, title_seq, abstract_seq, category_idx, subcategory_idx)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Store predictions and labels for metrics\n",
    "        all_predictions.extend(outputs.detach().cpu().numpy())\n",
    "        all_labels.extend(labels.detach().cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    auc = roc_auc_score(all_labels, all_predictions)\n",
    "    acc = accuracy_score(all_labels, [1 if p > 0.5 else 0 for p in all_predictions])\n",
    "    \n",
    "    return avg_loss, auc, acc\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            # Move batch to device\n",
    "            user_idx = batch['user_idx'].to(device)\n",
    "            title_seq = batch['title_seq'].to(device)\n",
    "            abstract_seq = batch['abstract_seq'].to(device)\n",
    "            category_idx = batch['category_idx'].to(device)\n",
    "            subcategory_idx = batch['subcategory_idx'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(user_idx, title_seq, abstract_seq, category_idx, subcategory_idx)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Store predictions and labels for metrics\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    auc = roc_auc_score(all_labels, all_predictions)\n",
    "    acc = accuracy_score(all_labels, [1 if p > 0.5 else 0 for p in all_predictions])\n",
    "    \n",
    "    return avg_loss, auc, acc\n",
    "\n",
    "# Initialize training components\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "print(\"Training components initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76899f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Training for 20 epochs\n",
      "------------------------------------------------------------\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:41<00:00,  8.19it/s]\n",
      "Evaluating:   0%|          | 0/85 [00:00<?, ?it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:06<00:00, 12.59it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6901, Train AUC: 0.5421, Train Acc: 0.5300\n",
      "Val Loss: 0.6799, Val AUC: 0.5959, Val Acc: 0.5713\n",
      "Learning Rate: 0.001000\n",
      "New best model! AUC: 0.5959\n",
      "------------------------------------------------------------\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:26<00:00, 12.83it/s]\n",
      "Evaluating:   0%|          | 0/85 [00:00<?, ?it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:03<00:00, 24.72it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:03<00:00, 24.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6781, Train AUC: 0.5975, Train Acc: 0.5709\n",
      "Val Loss: 0.6783, Val AUC: 0.6143, Val Acc: 0.5774\n",
      "Learning Rate: 0.001000\n",
      "New best model! AUC: 0.6143\n",
      "------------------------------------------------------------\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:29<00:00, 11.51it/s]\n",
      "Evaluating:   0%|          | 0/85 [00:00<?, ?it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:03<00:00, 25.76it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:03<00:00, 25.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6725, Train AUC: 0.6140, Train Acc: 0.5824\n",
      "Val Loss: 0.6677, Val AUC: 0.6244, Val Acc: 0.5894\n",
      "Learning Rate: 0.001000\n",
      "New best model! AUC: 0.6244\n",
      "------------------------------------------------------------\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:28<00:00, 11.95it/s]\n",
      "Evaluating:   0%|          | 0/85 [00:00<?, ?it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:03<00:00, 27.10it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:03<00:00, 27.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6663, Train AUC: 0.6306, Train Acc: 0.5934\n",
      "Val Loss: 0.6686, Val AUC: 0.6276, Val Acc: 0.5823\n",
      "Learning Rate: 0.001000\n",
      "New best model! AUC: 0.6276\n",
      "------------------------------------------------------------\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:39<00:00,  8.60it/s]\n",
      "Evaluating:   0%|          | 0/85 [00:00<?, ?it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:08<00:00, 10.55it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6637, Train AUC: 0.6358, Train Acc: 0.5965\n",
      "Val Loss: 0.6678, Val AUC: 0.6302, Val Acc: 0.5883\n",
      "Learning Rate: 0.001000\n",
      "New best model! AUC: 0.6302\n",
      "------------------------------------------------------------\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:38<00:00,  8.81it/s]\n",
      "Evaluating:   0%|          | 0/85 [00:00<?, ?it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:04<00:00, 20.09it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:04<00:00, 20.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6592, Train AUC: 0.6450, Train Acc: 0.6028\n",
      "Val Loss: 0.6638, Val AUC: 0.6337, Val Acc: 0.5898\n",
      "Learning Rate: 0.001000\n",
      "New best model! AUC: 0.6337\n",
      "------------------------------------------------------------\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:31<00:00, 10.56it/s]\n",
      "Evaluating:   0%|          | 0/85 [00:00<?, ?it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:03<00:00, 26.36it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6571, Train AUC: 0.6498, Train Acc: 0.6054\n",
      "Val Loss: 0.6642, Val AUC: 0.6323, Val Acc: 0.5919\n",
      "Learning Rate: 0.001000\n",
      "------------------------------------------------------------\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:39<00:00,  8.55it/s]\n",
      "\n",
      "Evaluating: 100%|██████████| 85/85 [00:04<00:00, 18.15it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6543, Train AUC: 0.6551, Train Acc: 0.6114\n",
      "Val Loss: 0.6633, Val AUC: 0.6360, Val Acc: 0.5974\n",
      "Learning Rate: 0.001000\n",
      "New best model! AUC: 0.6360\n",
      "------------------------------------------------------------\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:40<00:00,  8.43it/s]\n",
      "Evaluating:   0%|          | 0/85 [00:00<?, ?it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:04<00:00, 18.49it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6504, Train AUC: 0.6598, Train Acc: 0.6128\n",
      "Val Loss: 0.6649, Val AUC: 0.6356, Val Acc: 0.6002\n",
      "Learning Rate: 0.001000\n",
      "------------------------------------------------------------\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:44<00:00,  7.60it/s]\n",
      "Evaluating:   0%|          | 0/85 [00:00<?, ?it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:04<00:00, 18.67it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:04<00:00, 18.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6467, Train AUC: 0.6658, Train Acc: 0.6151\n",
      "Val Loss: 0.6668, Val AUC: 0.6348, Val Acc: 0.5977\n",
      "Learning Rate: 0.001000\n",
      "------------------------------------------------------------\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:38<00:00,  8.72it/s]\n",
      "\n",
      "Evaluating: 100%|██████████| 85/85 [00:04<00:00, 18.35it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:04<00:00, 18.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6454, Train AUC: 0.6691, Train Acc: 0.6182\n",
      "Val Loss: 0.6655, Val AUC: 0.6352, Val Acc: 0.5963\n",
      "Learning Rate: 0.001000\n",
      "------------------------------------------------------------\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:40<00:00,  8.34it/s]\n",
      "Evaluating:   0%|          | 0/85 [00:00<?, ?it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:04<00:00, 18.41it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:04<00:00, 18.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6404, Train AUC: 0.6753, Train Acc: 0.6205\n",
      "Val Loss: 0.6780, Val AUC: 0.6370, Val Acc: 0.6003\n",
      "Learning Rate: 0.000500\n",
      "New best model! AUC: 0.6370\n",
      "------------------------------------------------------------\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:48<00:00,  7.00it/s]\n",
      "Evaluating:   0%|          | 0/85 [00:00<?, ?it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:04<00:00, 19.00it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:04<00:00, 19.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6328, Train AUC: 0.6861, Train Acc: 0.6261\n",
      "Val Loss: 0.6680, Val AUC: 0.6374, Val Acc: 0.6002\n",
      "Learning Rate: 0.000500\n",
      "New best model! AUC: 0.6374\n",
      "------------------------------------------------------------\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:39<00:00,  8.49it/s]\n",
      "Evaluating:   0%|          | 0/85 [00:00<?, ?it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:02<00:00, 28.98it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:02<00:00, 28.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6299, Train AUC: 0.6928, Train Acc: 0.6337\n",
      "Val Loss: 0.6722, Val AUC: 0.6341, Val Acc: 0.5959\n",
      "Learning Rate: 0.000500\n",
      "------------------------------------------------------------\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:28<00:00, 11.84it/s]\n",
      "Evaluating:   0%|          | 0/85 [00:00<?, ?it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:03<00:00, 23.11it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:03<00:00, 23.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6284, Train AUC: 0.6930, Train Acc: 0.6345\n",
      "Val Loss: 0.6696, Val AUC: 0.6374, Val Acc: 0.5998\n",
      "Learning Rate: 0.000500\n",
      "------------------------------------------------------------\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:29<00:00, 11.46it/s]\n",
      "Evaluating:   0%|          | 0/85 [00:00<?, ?it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:03<00:00, 23.42it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:03<00:00, 23.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6257, Train AUC: 0.6983, Train Acc: 0.6357\n",
      "Val Loss: 0.6697, Val AUC: 0.6369, Val Acc: 0.6013\n",
      "Learning Rate: 0.000250\n",
      "------------------------------------------------------------\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:41<00:00,  8.06it/s]\n",
      "Evaluating:   0%|          | 0/85 [00:00<?, ?it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:03<00:00, 27.21it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:03<00:00, 27.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6198, Train AUC: 0.7059, Train Acc: 0.6405\n",
      "Val Loss: 0.6744, Val AUC: 0.6363, Val Acc: 0.6005\n",
      "Learning Rate: 0.000250\n",
      "------------------------------------------------------------\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:28<00:00, 11.78it/s]\n",
      "Evaluating:   0%|          | 0/85 [00:00<?, ?it/s]\n",
      "Evaluating: 100%|██████████| 85/85 [00:03<00:00, 24.70it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6174, Train AUC: 0.7110, Train Acc: 0.6480\n",
      "Val Loss: 0.6776, Val AUC: 0.6331, Val Acc: 0.5961\n",
      "Learning Rate: 0.000250\n",
      "Early stopping triggered after 5 epochs without improvement\n",
      "Loaded best model with validation AUC: 0.6374\n",
      "Training completed!\n",
      "Model saved as 'news_recommendation_cnn_model.pth'\n",
      "Best validation AUC: 0.6374\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "best_val_auc = 0\n",
    "best_model_state = None\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training for {num_epochs} epochs\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_auc, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_auc, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "        print(f\"New best model! AUC: {best_val_auc:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {patience} epochs without improvement\")\n",
    "        break\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"Loaded best model with validation AUC: {best_val_auc:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'word_dict': word_dict,\n",
    "    'category_dict': category_dict,\n",
    "    'subcategory_dict': subcategory_dict,\n",
    "    'uid2index': uid2index,\n",
    "    'best_val_auc': best_val_auc,\n",
    "    'model_config': {\n",
    "        'embed_dim': 100,\n",
    "        'num_filters': 128,\n",
    "        'filter_sizes': [3, 4, 5],\n",
    "        'user_embed_dim': 50,\n",
    "        'category_embed_dim': 20,\n",
    "        'dropout': 0.3\n",
    "    }\n",
    "}, 'cnn_model.pth')\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Model saved as 'cnn_model.pth'\")\n",
    "print(f\"Best validation AUC: {best_val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4f9ac6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available users (first 10):\n",
      " 1. U91836\n",
      " 2. U19739\n",
      " 3. U89744\n",
      " 4. U29155\n",
      " 5. U70879\n",
      " 6. U9306\n",
      " 7. U49572\n",
      " 8. U40466\n",
      " 9. U39029\n",
      "10. U22930\n",
      "\n",
      "Generating recommendations for User ID: U91836\n",
      "Loading model from cnn_model.pth...\n",
      "Model loaded successfully! Best validation AUC: 0.6374\n",
      "Preparing candidate news articles...\n",
      "Model loaded successfully! Best validation AUC: 0.6374\n",
      "Preparing candidate news articles...\n",
      "Computing recommendation scores...\n",
      "Computing recommendation scores...\n",
      "\n",
      "================================================================================\n",
      "TOP 10 NEWS RECOMMENDATIONS FOR USER: U91836\n",
      "================================================================================\n",
      " 1. Score: 0.8961 | music > musicnews\n",
      "    News ID: N49279\n",
      "    Title: Broadway Actress Laurel Griggs Dies at Age 13\n",
      "--------------------------------------------------------------------------------\n",
      " 2. Score: 0.8842 | travel > travelnews\n",
      "    News ID: N8373\n",
      "    Title: Video shows a Boeing 737 plane carrying 196 people burst into flames just after landing at an Egyptian airport\n",
      "--------------------------------------------------------------------------------\n",
      " 3. Score: 0.8522 | music > music-celebrity\n",
      "    News ID: N55943\n",
      "    Title: Wynonna Judd's Daughter, 23, Released from Prison 6 Years Early After Being Granted Parole\n",
      "--------------------------------------------------------------------------------\n",
      " 4. Score: 0.8463 | tv > tv-celebrity\n",
      "    News ID: N48705\n",
      "    Title: Lori Loughlin's 'Stress Is Only Mounting' Ahead of Pretrial Hearing\n",
      "--------------------------------------------------------------------------------\n",
      " 5. Score: 0.8423 | sports > football_ncaa\n",
      "    News ID: N40071\n",
      "    Title: Arkansas football coach Chad Morris out in middle of his second season\n",
      "--------------------------------------------------------------------------------\n",
      " 6. Score: 0.8336 | sports > football_nfl\n",
      "    News ID: N24837\n",
      "    Title: Robert Kraft reportedly told Rob Gronkowski he wanted him back for stretch run, but now it seems unlikely\n",
      "--------------------------------------------------------------------------------\n",
      " 7. Score: 0.8261 | news > newsworld\n",
      "    News ID: N32641\n",
      "    Title: U.S. Vows to Defend South Korea With Full Military Force Following Pyongyang Threats\n",
      "--------------------------------------------------------------------------------\n",
      " 8. Score: 0.8146 | tv > tv-celebrity\n",
      "    News ID: N9226\n",
      "    Title: Lori Loughlin's Daughters No Longer Enrolled at USC\n",
      "--------------------------------------------------------------------------------\n",
      " 9. Score: 0.8119 | sports > football_nfl\n",
      "    News ID: N60750\n",
      "    Title: Browns, Steelers brawl at end of Cleveland's 21-7 win\n",
      "--------------------------------------------------------------------------------\n",
      "10. Score: 0.8116 | news > newsworld\n",
      "    News ID: N350\n",
      "    Title: A Russian professor was pulled from a river. Police said a woman's arms were found in his rucksack\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "TOP 10 NEWS RECOMMENDATIONS FOR USER: U91836\n",
      "================================================================================\n",
      " 1. Score: 0.8961 | music > musicnews\n",
      "    News ID: N49279\n",
      "    Title: Broadway Actress Laurel Griggs Dies at Age 13\n",
      "--------------------------------------------------------------------------------\n",
      " 2. Score: 0.8842 | travel > travelnews\n",
      "    News ID: N8373\n",
      "    Title: Video shows a Boeing 737 plane carrying 196 people burst into flames just after landing at an Egyptian airport\n",
      "--------------------------------------------------------------------------------\n",
      " 3. Score: 0.8522 | music > music-celebrity\n",
      "    News ID: N55943\n",
      "    Title: Wynonna Judd's Daughter, 23, Released from Prison 6 Years Early After Being Granted Parole\n",
      "--------------------------------------------------------------------------------\n",
      " 4. Score: 0.8463 | tv > tv-celebrity\n",
      "    News ID: N48705\n",
      "    Title: Lori Loughlin's 'Stress Is Only Mounting' Ahead of Pretrial Hearing\n",
      "--------------------------------------------------------------------------------\n",
      " 5. Score: 0.8423 | sports > football_ncaa\n",
      "    News ID: N40071\n",
      "    Title: Arkansas football coach Chad Morris out in middle of his second season\n",
      "--------------------------------------------------------------------------------\n",
      " 6. Score: 0.8336 | sports > football_nfl\n",
      "    News ID: N24837\n",
      "    Title: Robert Kraft reportedly told Rob Gronkowski he wanted him back for stretch run, but now it seems unlikely\n",
      "--------------------------------------------------------------------------------\n",
      " 7. Score: 0.8261 | news > newsworld\n",
      "    News ID: N32641\n",
      "    Title: U.S. Vows to Defend South Korea With Full Military Force Following Pyongyang Threats\n",
      "--------------------------------------------------------------------------------\n",
      " 8. Score: 0.8146 | tv > tv-celebrity\n",
      "    News ID: N9226\n",
      "    Title: Lori Loughlin's Daughters No Longer Enrolled at USC\n",
      "--------------------------------------------------------------------------------\n",
      " 9. Score: 0.8119 | sports > football_nfl\n",
      "    News ID: N60750\n",
      "    Title: Browns, Steelers brawl at end of Cleveland's 21-7 win\n",
      "--------------------------------------------------------------------------------\n",
      "10. Score: 0.8116 | news > newsworld\n",
      "    News ID: N350\n",
      "    Title: A Russian professor was pulled from a river. Police said a woman's arms were found in his rucksack\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation: Personalized News Recommendations\n",
    "def recommend_for_user(user_id, model_path, news_data, word_dict, category_dict, subcategory_dict, uid2index, device, top_k=10):\n",
    "    \"\"\"Recommend top news articles for a given user_id using the trained CNN model.\"\"\"\n",
    "    \n",
    "    # Check if user exists\n",
    "    if user_id not in uid2index:\n",
    "        print(f\"User_ID {user_id} not found in the dataset.\")\n",
    "        return []\n",
    "    \n",
    "    # Load the trained model\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    model_config = checkpoint['model_config']\n",
    "    \n",
    "    # Initialize model with saved configuration\n",
    "    model = NewsRecommendationCNN(\n",
    "        embedding_matrix=embedding_matrix,\n",
    "        num_users=len(uid2index),\n",
    "        num_categories=len(category_dict),\n",
    "        num_subcategories=len(subcategory_dict),\n",
    "        **model_config\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(f\"Model loaded successfully! Best validation AUC: {checkpoint['best_val_auc']:.4f}\")\n",
    "    \n",
    "    user_idx = uid2index[user_id]\n",
    "    \n",
    "    # Prepare all news articles as candidates\n",
    "    print(\"Preparing candidate news articles...\")\n",
    "    candidates = []\n",
    "    for _, news_row in news_data.iterrows():\n",
    "        news_id = news_row['News_ID']\n",
    "        title_seq = text_to_sequence(news_row['Title'], word_dict, max_len=30)\n",
    "        abstract_seq = text_to_sequence(news_row['Abstract'], word_dict, max_len=50)\n",
    "        category_idx = category_dict.get(news_row['Category'], 0)\n",
    "        subcategory_idx = subcategory_dict.get(news_row['SubCategory'], 0)\n",
    "        \n",
    "        candidates.append({\n",
    "            'news_id': news_id,\n",
    "            'title': news_row['Title'],\n",
    "            'category': news_row['Category'],\n",
    "            'subcategory': news_row['SubCategory'],\n",
    "            'title_seq': title_seq,\n",
    "            'abstract_seq': abstract_seq,\n",
    "            'category_idx': category_idx,\n",
    "            'subcategory_idx': subcategory_idx\n",
    "        })\n",
    "    \n",
    "    # Batch inference for efficiency\n",
    "    print(\"Computing recommendation scores...\")\n",
    "    batch_size = 128\n",
    "    scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(candidates), batch_size):\n",
    "            batch = candidates[i:i+batch_size]\n",
    "            batch_size_actual = len(batch)\n",
    "            \n",
    "            # Prepare batch tensors\n",
    "            user_idxs = torch.tensor([user_idx] * batch_size_actual, dtype=torch.long, device=device)\n",
    "            title_seqs = torch.tensor([c['title_seq'] for c in batch], dtype=torch.long, device=device)\n",
    "            abstract_seqs = torch.tensor([c['abstract_seq'] for c in batch], dtype=torch.long, device=device)\n",
    "            category_idxs = torch.tensor([c['category_idx'] for c in batch], dtype=torch.long, device=device)\n",
    "            subcategory_idxs = torch.tensor([c['subcategory_idx'] for c in batch], dtype=torch.long, device=device)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(user_idxs, title_seqs, abstract_seqs, category_idxs, subcategory_idxs)\n",
    "            scores.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    # Combine news with their scores and sort by score (descending)\n",
    "    news_with_scores = []\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        news_with_scores.append({\n",
    "            'news_id': candidate['news_id'],\n",
    "            'title': candidate['title'],\n",
    "            'category': candidate['category'],\n",
    "            'subcategory': candidate['subcategory'],\n",
    "            'score': scores[i]\n",
    "        })\n",
    "    \n",
    "    # Sort by score in descending order\n",
    "    news_with_scores.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Display top recommendations\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"TOP {top_k} NEWS RECOMMENDATIONS FOR USER: {user_id}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, news in enumerate(news_with_scores[:top_k], 1):\n",
    "        print(f\"{i:2d}. Score: {news['score']:.4f} | {news['category']} > {news['subcategory']}\")\n",
    "        print(f\"    News ID: {news['news_id']}\")\n",
    "        print(f\"    Title: {news['title']}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return news_with_scores[:top_k]\n",
    "\n",
    "# Example: Get recommendations for a sample user\n",
    "print(\"Available users (first 10):\")\n",
    "sample_users = [u for u in uid2index.keys() if isinstance(u, str) and u.strip() and u.lower() != 'user_id'][:10]\n",
    "for i, user in enumerate(sample_users, 1):\n",
    "    print(f\"{i:2d}. {user}\")\n",
    "\n",
    "# Use the first user as an example\n",
    "sample_user_id = sample_users[0]\n",
    "print(f\"\\nGenerating recommendations for User ID: {sample_user_id}\")\n",
    "\n",
    "# Get recommendations\n",
    "recommendations = recommend_for_user(\n",
    "    user_id=sample_user_id, \n",
    "    model_path='cnn_model.pth', \n",
    "    news_data=news_data, \n",
    "    word_dict=word_dict, \n",
    "    category_dict=category_dict, \n",
    "    subcategory_dict=subcategory_dict, \n",
    "    uid2index=uid2index, \n",
    "    device=device, \n",
    "    top_k=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
